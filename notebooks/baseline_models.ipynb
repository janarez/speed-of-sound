{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models training\n",
    "\n",
    "The main purspose of this notebook is to explore baseline models. This is done in the second part of the notebook. In the first part of the notebook standard normalization of all temperature and molality related features is performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings in this notebook\n",
    "# not necessary, can be commented\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for loading datasets\n",
    "path = r\"../datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading enhanced electrolyte dataset\n",
    "dataframe = pd.read_csv(path + \"dataset.csv\", index_col=0)\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading enhanced RbI dataset\n",
    "try:\n",
    "    outtest = pd.read_csv(path + \"outtest_dataset.csv\", index_col=0)\n",
    "    print(outtest.shape)\n",
    "except FileNotFoundError:\n",
    "        print(\"The CSV file must be first created in features_addition notebook.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test data splitting\n",
    "Shuffled (to distribute media randomly) data from dataframe are split into three sets in the following propotions:\n",
    " - training data: 70 %\n",
    " - validation data: 20 %\n",
    " - test data: 10 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data to distribute electrolytes randomly\n",
    "dataframe = dataframe.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = list(dataframe.columns[:-2])    # feature names (excludes \"medium\" and \"sound\" columns)\n",
    "\n",
    "X = dataframe[X_cols]            # store model features dataframe\n",
    "y = dataframe[\"sound\"]            # store sound speed values series\n",
    "\n",
    "media = dataframe[\"medium\"]              # store list of media\n",
    "T = dataframe[\"T\"].copy()                # remember original T feature for use in model_evaluation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies for data splitting\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test spliting\n",
    "# 70 % train, 20 % val, 10 % test\n",
    "\n",
    "X_train, X_tv, y_train, y_tv = train_test_split(X, y, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tv, y_tv, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the test, validation, train set sizes\n",
    "print(\"Number of samples in train set: {0}, {1:.0f} % of total\"\n",
    "      .format(len(y_train), 100 * len(y_train) / dataframe.shape[0]))\n",
    "\n",
    "print(\"Number of samples in validation set: {0}, {1:.0f} % of total\"\n",
    "      .format(len(y_val), 100 * len(y_val) / dataframe.shape[0]))\n",
    "\n",
    "print(\"Number of samples in test set: {0}, {1:.0f} % of total\"\n",
    "      .format(len(y_test), 100 * len(y_test) / dataframe.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard normalization of temperature and molality related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the standard normalization object\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "norm = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize yet non scaled features: kW, T, is, c, cm\n",
    "to_norm = [\"T\", \"c\",\"Kw\",\"cm\", \"is\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only fit the normalization on training data\n",
    "norm.fit(X_train[to_norm])\n",
    "\n",
    "# transform all sets\n",
    "X_train[to_norm] = norm.transform(X_train[to_norm])\n",
    "X_val[to_norm] = norm.transform(X_val[to_norm])\n",
    "X_test[to_norm] = norm.transform(X_test[to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that standard normalization went well: std = 1, mean = 0\n",
    "X_train[to_norm].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed dataset, ready for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed dataset, must normalize it first\n",
    "dataframe[to_norm] = norm.transform(dataframe[to_norm])\n",
    "# add original temperature values for plotting in model_evaluation notebook\n",
    "dataframe[\"T_orig\"] = T\n",
    "dataframe.to_csv(r\"../datasets/preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed outside matrix RbI test dataset\n",
    "\n",
    "try:\n",
    "    outtest[to_norm] = norm.transform(outtest[to_norm])\n",
    "    outtest.to_csv(r\"../datasets/o_preprocessed.csv\")\n",
    "except:\n",
    "    print(\"The outtest dataframe does not exist.\")\n",
    "    print(\"Do not forget to enhance the RbI data in features_addition notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models\n",
    "The following three models were proposed and tested as baseline models:\n",
    " - Decision tree: maximum depth 10\n",
    " - Elastic net: on original feature set\n",
    " - Elastic net: on 2nd order polynomial transformation of feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation metric function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 and rmse and AARD returning function\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def result_stats(actual, predicted):\n",
    "    \"\"\"\n",
    "    Returns r_2, rmse and AARD value for two arrays of equal length\n",
    "    \"\"\"\n",
    "    \n",
    "    r2 = r2_score(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error( actual, predicted ))\n",
    "    aard = (100 / len(actual)) * np.sum(np.abs((actual - predicted) / actual))\n",
    "    \n",
    "    return r2,rmse, aard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Baseline models import__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(max_depth=10)  # limit tree depth to 10\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "en = ElasticNet()  # all hyperparameters are default\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pr = PolynomialFeatures(degree=2, interaction_only=False)  # all possible 2nd order combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the baseline models using 5 fold cross validation on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_baseline(model, c, X, y):\n",
    "    \"\"\"\n",
    "    Five fold cross validation run on model using feature data X with target value y.\n",
    "    Prints model RMSE.\n",
    "    \"\"\"\n",
    "    pred = cross_val_predict(model, X, y, cv=5)\n",
    "    _, rmse, _ = result_stats(y, pred)\n",
    "    \n",
    "    print('{1} CV Avg.: {0:.1f}'.format(rmse, model.__str__().split(\"(\",1)[0]))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "cross_validate_baseline(dt, \"b\", X_train, y_train)\n",
    "\n",
    "# elastic net linear\n",
    "cross_validate_baseline(en, \"g\", X_train, y_train)\n",
    "\n",
    "# polynomial transformation of feature set\n",
    "X_pr = pr.fit_transform(X_train)\n",
    "# elastic net polynomial\n",
    "cross_validate_baseline(en, \"r\", X_pr, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results of baseline models on validation data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit decision tree on train data, use validation set for evaluation\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_val)\n",
    "print(\"Validation results for Decision tree: RMSE {0:.1f}\".format(result_stats(y_val, dt_pred)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit elastic net on train data, use validation set for evaluation\n",
    "en.fit(X_train, y_train)\n",
    "en_pred = en.predict(X_val)\n",
    "print(\"Validation results for Elastic net with linear features: RMSE {0:.1f}\".format(result_stats(y_val, en_pred)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do 2nd order polynomial transformation of input data first\n",
    "pr.fit(X_train)\n",
    "X_train_pr = pr.transform(X_train)\n",
    "X_val_pr = pr.transform(X_val)\n",
    "\n",
    "# fit elastic net on transformed train data, use transformed validation set for evaluation\n",
    "en.fit(X_train_pr, y_train)\n",
    "pr_pred = en.predict(X_val_pr)\n",
    "print(\"Validation results for Elastic net with polynomial features: RMSE {0:.1f}\"\n",
    "      .format(result_stats(y_val, pr_pred)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual plots for all baseline models\n",
    "# the second plot has the y-axis values limited\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n",
    "\n",
    "for i in [0,1]:            \n",
    "    \n",
    "    ax[i].scatter(x = range(len(y_val)),y = dt_pred-y_val,\n",
    "                  alpha=0.2,c=\"b\", label = \"Decision tree\")     # decision tree - blue\n",
    "    \n",
    "    ax[i].scatter(x=range(len(y_val)),y = en_pred - y_val, \n",
    "                  alpha=0.2, c=\"g\", label = \"Elastic net (1)\")   # linear elastic net - green\n",
    "    \n",
    "    ax[i].scatter(x=range(len(y_val)),y=pr_pred-y_val,\n",
    "                  alpha=0.2,c=\"y\", label = \"Elastic net (2)\")         # polynomial elastic net - yellow\n",
    "    \n",
    "    ax[i].set_xlabel(\"Data point ID\")\n",
    "    ax[i].set_ylabel(\"Residual $u$ [m/s]\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].set_ylim(-50,50)\n",
    "fig.suptitle(\"Baseline model residual plots on validation data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the train, validation and test data for use in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store X_train\n",
    "%store y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store X_val\n",
    "%store y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store X_test\n",
    "%store y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
